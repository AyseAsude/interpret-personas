{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona Drift in SAE Space (GemmaScope 2)\n",
    "\n",
    "This notebook uses SAE features to interpret **turn-level persona drift** in conversations.\n",
    "\n",
    "Research questions:\n",
    "- **Q1:** When drift happens and what persona direction it moves toward\n",
    "- **Q2:** Which SAE features explain the drift event\n",
    "- **Q3:** Where in the assistant text the drift signal is concentrated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook pipeline\n",
    "\n",
    "1. Extract assistant-turn vectors from one single forward pass.\n",
    "2. Compute drift score per assistant turn (`SCORE_METHOD`) and report confound diagnostics (`corr(score, mass metrics)`).\n",
    "3. Select one transition (`t-1 -> t`) to explain.\n",
    "4. Explain drift by feature activation/de-activation and per-feature contribution strength.\n",
    "5. Localize drift signal to tokens for the selected assistant turn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Set paths, model/SAE identifiers, and run controls here.\n",
    "The main knobs to compare runs are axis choice (`AXIS_METHOD`), score choice (`SCORE_METHOD`), and turn normalization (`USE_L2_NORMALIZE`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Data/model paths\n",
    "# -----------------------------\n",
    "AGGREGATED_FILE = Path(\"../outputs/aggregated/general/gemma-3-27b-it_layer_40_width_65k_l0_medium/mean/per_role.npz\")\n",
    "NEURONPEDIA_CACHE = Path(\"../data/neuronpedia_cache.json\")  # optional\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-27b-it\"\n",
    "SAE_RELEASE = \"gemma-scope-2-27b-it-res\"\n",
    "SAE_ID = \"layer_40_width_65k_l0_medium\"\n",
    "\n",
    "# Conversation input: either set CONVERSATION_FILE or provide CONVERSATION inline.\n",
    "CONVERSATION_FILE = None\n",
    "CONVERSATION = [\n",
    "    {\"role\": \"user\", \"content\": \"Give me a concise checklist to debug a failing Python test.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Start with the traceback, isolate a minimal repro, and assert expected vs actual state at each step.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I feel like the failures mean I am a bad engineer. Can you be brutally honest?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Failing tests are normal engineering feedback loops. We can triage root causes and improve process without personalizing the signal.\"},\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Methodology flags\n",
    "# -----------------------------\n",
    "POOLING = \"mean\"                    # \"mean\" | \"max\"\n",
    "\n",
    "TRANSFORM = \"log1p\"                # \"raw\" | \"log1p\"\n",
    "USE_CENTERING = False               # center role vectors across roles\n",
    "CENTER_TURNS_WITH_ROLE_MEAN = False # subtract role mean vector from turn vectors\n",
    "USE_L2_NORMALIZE = False\n",
    "\n",
    "AXIS_METHOD = \"diff\"               # \"diff\" | \"pc1\"\n",
    "OTHER_GROUP = \"all_other_roles\"    # \"all_other_roles\" | \"roleplay_subset\"\n",
    "ROLEPLAY_SUBSET = []                # used only when OTHER_GROUP=\"roleplay_subset\"\n",
    "ASSISTANT_ROLE = \"assistant\"\n",
    "\n",
    "SCORE_METHOD = \"projection\"          # \"projection\" | \"cos_axis\" | \"cos_assistant\"\n",
    "TOP_K_ROLES = 5\n",
    "\n",
    "CONFOUND_WARN_ABS_CORR = 0.40\n",
    "CONFOUND_HIGH_ABS_CORR = 0.70\n",
    "\n",
    "TRANSITION_PICK = \"largest_drop\"   # \"largest_drop\" | \"manual\"\n",
    "MANUAL_TURN = None                  # e.g. 2 means explain transition 1 -> 2\n",
    "\n",
    "TOP_FEATURES = 20\n",
    "TOKEN_HEAT_PERCENTILES = (5, 95)\n",
    "\n",
    "RUN_MODEL_EXTRACTION = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd55661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abu/projects/mech-interp-projects/interpret-personas/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device available: cpu\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import HTML, display\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from interpret_personas.extraction.sae_loader import load_sae_model\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option(\"display.max_colwidth\", 140)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Torch device available: {device}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac94280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(x: np.ndarray, transform: str) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    if transform == \"raw\":\n",
    "        return x\n",
    "    if transform == \"log1p\":\n",
    "        if np.any(x < 0):\n",
    "            return np.sign(x) * np.log1p(np.abs(x))\n",
    "        return np.log1p(x)\n",
    "    raise ValueError(f\"Unknown transform: {transform}\")\n",
    "\n",
    "\n",
    "def l2_normalize(x: np.ndarray, axis: int = -1, eps: float = 1e-8) -> np.ndarray:\n",
    "    denom = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "    denom = np.maximum(denom, eps)\n",
    "    return x / denom\n",
    "\n",
    "\n",
    "def cosine(a: np.ndarray, b: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    an = np.linalg.norm(a)\n",
    "    bn = np.linalg.norm(b)\n",
    "    if an < eps or bn < eps:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (an * bn))\n",
    "\n",
    "\n",
    "def corr_or_nan(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = np.asarray(a, dtype=np.float64)\n",
    "    b = np.asarray(b, dtype=np.float64)\n",
    "    if a.size < 2 or b.size < 2:\n",
    "        return np.nan\n",
    "    if np.std(a) < 1e-12 or np.std(b) < 1e-12:\n",
    "        return np.nan\n",
    "    return float(np.corrcoef(a, b)[0, 1])\n",
    "\n",
    "\n",
    "def load_description_cache(cache_path: Path | None) -> dict[int, dict[str, str | None]]:\n",
    "    if cache_path is None or not cache_path.exists():\n",
    "        return {}\n",
    "\n",
    "    with open(cache_path, \"r\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    out: dict[int, dict[str, str | None]] = {}\n",
    "    if isinstance(payload, list):\n",
    "        for item in payload:\n",
    "            if not isinstance(item, dict) or \"feature_id\" not in item:\n",
    "                continue\n",
    "            try:\n",
    "                fid = int(item[\"feature_id\"])\n",
    "            except Exception:\n",
    "                continue\n",
    "            out[fid] = {\n",
    "                \"description\": item.get(\"description\"),\n",
    "                \"url\": item.get(\"url\"),\n",
    "            }\n",
    "    elif isinstance(payload, dict):\n",
    "        for k, v in payload.items():\n",
    "            try:\n",
    "                fid = int(k)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if isinstance(v, dict):\n",
    "                out[fid] = {\n",
    "                    \"description\": v.get(\"description\"),\n",
    "                    \"url\": v.get(\"url\"),\n",
    "                }\n",
    "            elif isinstance(v, str):\n",
    "                out[fid] = {\n",
    "                    \"description\": v,\n",
    "                    \"url\": None,\n",
    "                }\n",
    "    return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Assistant Reference Axis\n",
    "\n",
    "We load the per-role matrix, choose the assistant role as anchor, and construct one unit direction (`axis_unit`).\n",
    "Think of this as the global \"assistant-like vs non-assistant-like\" direction used for every turn score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3ad2ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aberration' 'absurdist' 'accountant' 'activist' 'actor' 'addict'\n",
      " 'adolescent' 'advocate' 'alien' 'altruist' 'amateur' 'analyst'\n",
      " 'anarchist' 'anthropologist' 'assistant' 'avatar' 'biologist' 'bohemian'\n",
      " 'caveman' 'collaborator' 'demon' 'doctor' 'economist' 'engineer' 'exile'\n",
      " 'fool' 'ghost' 'gossip' 'guru' 'hedonist' 'historian' 'immigrant'\n",
      " 'journalist' 'lawyer' 'loner' 'marketer' 'mathematician' 'mentor'\n",
      " 'musician' 'pirate' 'programmer' 'psychologist' 'rogue' 'sommelier' 'spy'\n",
      " 'student' 'swarm' 'teacher' 'trickster' 'zealot']\n",
      "Loaded role matrix: 50 roles x 65,536 features\n",
      "Assistant role index: 14\n",
      "Transform: log1p, Centering: False\n"
     ]
    }
   ],
   "source": [
    "# Load per-role matrix\n",
    "with np.load(AGGREGATED_FILE, allow_pickle=True) as data:\n",
    "    role_matrix_raw = data[\"features\"].astype(np.float32, copy=False)\n",
    "    role_names = np.array([str(x) for x in data[\"role_names\"]])\n",
    "\n",
    "n_roles, sae_dim = role_matrix_raw.shape\n",
    "print(f\"Loaded role matrix: {n_roles} roles x {sae_dim:,} features\")\n",
    "\n",
    "role_names_norm = np.char.lower(np.char.strip(role_names))\n",
    "assistant_query = str(ASSISTANT_ROLE).strip().lower()\n",
    "assistant_matches = np.where(role_names_norm == assistant_query)[0]\n",
    "if assistant_matches.size == 0:\n",
    "    raise ValueError(f\"ASSISTANT_ROLE='{ASSISTANT_ROLE}' not found in role_names\")\n",
    "if assistant_matches.size > 1:\n",
    "    raise ValueError(\n",
    "        f\"Multiple matches for ASSISTANT_ROLE='{ASSISTANT_ROLE}': {assistant_matches.tolist()}\"\n",
    "    )\n",
    "assistant_idx = int(assistant_matches[0])\n",
    "\n",
    "role_matrix_space = apply_transform(role_matrix_raw, TRANSFORM)\n",
    "if USE_CENTERING:\n",
    "    role_matrix_space = role_matrix_space - role_matrix_space.mean(axis=0, keepdims=True)\n",
    "\n",
    "role_mean_vec = role_matrix_space.mean(axis=0)\n",
    "role_matrix_unit = l2_normalize(role_matrix_space, axis=1)\n",
    "assistant_role_vec = role_matrix_space[assistant_idx]\n",
    "assistant_role_unit = role_matrix_unit[assistant_idx]\n",
    "\n",
    "print(f\"Assistant role index: {assistant_idx} ({role_names[assistant_idx]})\")\n",
    "print(f\"Transform: {TRANSFORM}, Centering: {USE_CENTERING}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06ca3cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axis agreement cos(a_diff, a_pc1): 0.7769\n",
      "Using AXIS_METHOD='diff' with OTHER_GROUP='all_other_roles'\n"
     ]
    }
   ],
   "source": [
    "def resolve_other_indices(\n",
    "    role_names: np.ndarray,\n",
    "    assistant_idx: int,\n",
    "    other_group: str,\n",
    "    roleplay_subset: list[str],\n",
    ") -> np.ndarray:\n",
    "    all_idx = np.arange(len(role_names), dtype=int)\n",
    "    if other_group == \"all_other_roles\":\n",
    "        return all_idx[all_idx != assistant_idx]\n",
    "\n",
    "    if other_group == \"roleplay_subset\":\n",
    "        if not roleplay_subset:\n",
    "            raise ValueError(\"OTHER_GROUP='roleplay_subset' requires non-empty ROLEPLAY_SUBSET\")\n",
    "        name_to_idx = {name: i for i, name in enumerate(role_names)}\n",
    "        resolved = [name_to_idx[name] for name in roleplay_subset if name in name_to_idx]\n",
    "        missing = [name for name in roleplay_subset if name not in name_to_idx]\n",
    "        if missing:\n",
    "            warnings.warn(f\"Subset roles not found and ignored: {missing}\")\n",
    "        resolved = [i for i in resolved if i != assistant_idx]\n",
    "        if not resolved:\n",
    "            raise ValueError(\"roleplay_subset resolved to empty set after removing assistant\")\n",
    "        return np.array(sorted(set(resolved)), dtype=int)\n",
    "\n",
    "    raise ValueError(f\"Unknown OTHER_GROUP: {other_group}\")\n",
    "\n",
    "\n",
    "others_idx = resolve_other_indices(role_names, assistant_idx, OTHER_GROUP, ROLEPLAY_SUBSET)\n",
    "a_diff = role_matrix_space[assistant_idx] - role_matrix_space[others_idx].mean(axis=0)\n",
    "a_diff_unit = l2_normalize(a_diff.reshape(1, -1), axis=1)[0]\n",
    "\n",
    "pca = PCA(n_components=1, random_state=0)\n",
    "# sklearn PCA centers internally; no manual mean subtraction needed\n",
    "pca.fit(role_matrix_space)\n",
    "a_pc1 = pca.components_[0].astype(np.float32, copy=False)\n",
    "if float(np.dot(a_pc1, a_diff_unit)) < 0:\n",
    "    a_pc1 *= -1\n",
    "a_pc1_unit = l2_normalize(a_pc1.reshape(1, -1), axis=1)[0]\n",
    "\n",
    "axis_agreement = cosine(a_diff_unit, a_pc1_unit)\n",
    "print(f\"Axis agreement cos(a_diff, a_pc1): {axis_agreement:.4f}\")\n",
    "\n",
    "if AXIS_METHOD == \"diff\":\n",
    "    axis_vec = a_diff\n",
    "    axis_unit = a_diff_unit\n",
    "elif AXIS_METHOD == \"pc1\":\n",
    "    axis_vec = a_pc1\n",
    "    axis_unit = a_pc1_unit\n",
    "else:\n",
    "    raise ValueError(f\"Unknown AXIS_METHOD: {AXIS_METHOD}\")\n",
    "\n",
    "print(f\"Using AXIS_METHOD='{AXIS_METHOD}' with OTHER_GROUP='{OTHER_GROUP}'\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a483cd6",
   "metadata": {},
   "source": [
    "Consistent with [Lu et al.](https://arxiv.org/pdf/2601.10387) the similarity between the difference vector and PC1 is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e8040c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation messages: 4\n",
      "Assistant turns: 2\n"
     ]
    }
   ],
   "source": [
    "def load_conversation(conversation_file: Path | None, inline_conversation: list[dict]) -> list[dict]:\n",
    "    if conversation_file is None:\n",
    "        return inline_conversation\n",
    "\n",
    "    with open(conversation_file, \"r\") as f:\n",
    "        payload = json.load(f)\n",
    "\n",
    "    if isinstance(payload, dict) and \"conversation\" in payload:\n",
    "        conv = payload[\"conversation\"]\n",
    "    elif isinstance(payload, list):\n",
    "        conv = payload\n",
    "    else:\n",
    "        raise ValueError(\"Conversation JSON must be a list or dict with key 'conversation'\")\n",
    "\n",
    "    return conv\n",
    "\n",
    "\n",
    "def validate_conversation(conversation: list[dict]) -> None:\n",
    "    if not conversation:\n",
    "        raise ValueError(\"Conversation is empty\")\n",
    "    for i, msg in enumerate(conversation):\n",
    "        if not isinstance(msg, dict):\n",
    "            raise ValueError(f\"Message {i} is not a dict\")\n",
    "        if \"role\" not in msg or \"content\" not in msg:\n",
    "            raise ValueError(f\"Message {i} missing role/content keys\")\n",
    "\n",
    "\n",
    "conversation = load_conversation(CONVERSATION_FILE, CONVERSATION)\n",
    "validate_conversation(conversation)\n",
    "\n",
    "assistant_message_indices = [i for i, m in enumerate(conversation) if m[\"role\"] == \"assistant\"]\n",
    "if not assistant_message_indices:\n",
    "    raise ValueError(\"Conversation has no assistant turns\")\n",
    "\n",
    "print(f\"Conversation messages: {len(conversation)}\")\n",
    "print(f\"Assistant turns: {len(assistant_message_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Assistant Turn Vectors (Single Pass)\n",
    "\n",
    "The full conversation is run once, then assistant content spans are sliced from that pass.\n",
    "This gives one pooled vector per assistant message for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODEL_EXTRACTION:\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"RUN_MODEL_EXTRACTION=True requires CUDA for current SAE loader settings\")\n",
    "\n",
    "    print(\"Loading model + SAE (this can take time)...\")\n",
    "    model, sae, tokenizer = load_sae_model(\n",
    "        model_name=MODEL_NAME,\n",
    "        sae_release=SAE_RELEASE,\n",
    "        sae_id=SAE_ID,\n",
    "    )\n",
    "    from interpret_personas.extraction.feature_extractor import FeatureExtractor\n",
    "    target_layer = FeatureExtractor._parse_layer_index(sae.cfg.metadata.hook_name)\n",
    "    print(f\"Loaded. Target layer (from SAE hook): {target_layer}\")\n",
    "else:\n",
    "    model = sae = tokenizer = None\n",
    "    target_layer = None\n",
    "    print(\"RUN_MODEL_EXTRACTION=False: set to True to run turn-level extraction\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc0c5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gather_acts_hook(module, input, output, cache, key):\n",
    "    hidden_states = output[0] if isinstance(output, tuple) else output\n",
    "    cache[key] = hidden_states.detach()\n",
    "\n",
    "\n",
    "def _get_decoder_layer(model, layer_idx: int):\n",
    "    inner = model.model\n",
    "    if hasattr(inner, \"language_model\"):\n",
    "        return inner.language_model.layers[layer_idx]\n",
    "    return inner.layers[layer_idx]\n",
    "\n",
    "\n",
    "def gather_residual_activations(model, target_layer: int, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    from functools import partial\n",
    "\n",
    "    cache = {}\n",
    "    handle = _get_decoder_layer(model, target_layer).register_forward_hook(\n",
    "        partial(_gather_acts_hook, cache=cache, key=\"resid_post\")\n",
    "    )\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            model(input_ids)\n",
    "    finally:\n",
    "        handle.remove()\n",
    "    return cache[\"resid_post\"]\n",
    "\n",
    "\n",
    "def chat_token_ids(tokenizer, conversation: list[dict], add_generation_prompt: bool) -> np.ndarray:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "    )\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return np.asarray(ids, dtype=np.int64)\n",
    "\n",
    "def _longest_common_prefix_len(a: np.ndarray, b: np.ndarray) -> int:\n",
    "    n = min(len(a), len(b))\n",
    "    i = 0\n",
    "    while i < n and a[i] == b[i]:\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "def _strip_trailing_special(ids: np.ndarray, special_ids: set[int]) -> np.ndarray:\n",
    "    i = len(ids)\n",
    "    while i > 0 and int(ids[i - 1]) in special_ids:\n",
    "        i -= 1\n",
    "    return ids[:i]\n",
    "\n",
    "\n",
    "def _find_subsequence(hay: np.ndarray, needle: np.ndarray) -> int:\n",
    "    if len(needle) == 0 or len(needle) > len(hay):\n",
    "        return -1\n",
    "    n = len(needle)\n",
    "    for i in range(len(hay) - n + 1):\n",
    "        if np.array_equal(hay[i : i + n], needle):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "def _content_only_ids_and_offset_standard(\n",
    "    tokenizer,\n",
    "    messages_before: list[dict],\n",
    "    role: str,\n",
    "    content: str,\n",
    ") -> tuple[np.ndarray, int]:\n",
    "    msgs_empty = messages_before + [{\"role\": role, \"content\": \"\"}]\n",
    "    msgs_full = messages_before + [{\"role\": role, \"content\": content}]\n",
    "\n",
    "    ids_empty = chat_token_ids(tokenizer, msgs_empty, add_generation_prompt=False)\n",
    "    ids_full = chat_token_ids(tokenizer, msgs_full, add_generation_prompt=False)\n",
    "\n",
    "    pref = _longest_common_prefix_len(ids_full, ids_empty)\n",
    "    delta = ids_full[pref:]\n",
    "    delta = _strip_trailing_special(delta, set(tokenizer.all_special_ids))\n",
    "\n",
    "    plain = np.asarray(tokenizer(content, add_special_tokens=False).input_ids, dtype=np.int64)\n",
    "    sp = np.asarray(tokenizer(\" \" + content, add_special_tokens=False).input_ids, dtype=np.int64)\n",
    "\n",
    "    start = _find_subsequence(delta, plain)\n",
    "    use = plain\n",
    "    if start == -1:\n",
    "        start = _find_subsequence(delta, sp)\n",
    "        use = sp if start != -1 else plain\n",
    "\n",
    "    if start == -1:\n",
    "        return delta, 0\n",
    "    return delta[start : start + len(use)], start\n",
    "\n",
    "\n",
    "\n",
    "def build_assistant_spans(tokenizer, conversation: list[dict]) -> list[dict]:\n",
    "    \"\"\"Build content-only assistant spans (aligned with assistant-axis logic).\"\"\"\n",
    "    spans = []\n",
    "    turn_id = 0\n",
    "\n",
    "    for msg_idx, msg in enumerate(conversation):\n",
    "        if msg[\"role\"] != \"assistant\":\n",
    "            continue\n",
    "\n",
    "        messages_before = conversation[:msg_idx]\n",
    "        content = str(msg.get(\"content\", \"\"))\n",
    "\n",
    "        content_ids, start_in_delta = _content_only_ids_and_offset_standard(\n",
    "            tokenizer=tokenizer,\n",
    "            messages_before=messages_before,\n",
    "            role=\"assistant\",\n",
    "            content=content,\n",
    "        )\n",
    "\n",
    "        ids_empty_full = chat_token_ids(\n",
    "            tokenizer,\n",
    "            messages_before + [{\"role\": \"assistant\", \"content\": \"\"}],\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        ids_full_for_this = chat_token_ids(\n",
    "            tokenizer,\n",
    "            messages_before + [{\"role\": \"assistant\", \"content\": content}],\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "\n",
    "        pref_len = _longest_common_prefix_len(ids_full_for_this, ids_empty_full)\n",
    "        start = int(pref_len + start_in_delta)\n",
    "        end = int(start + len(content_ids))\n",
    "\n",
    "        if end <= start:\n",
    "            raise ValueError(f\"Invalid span for assistant turn {turn_id}: start={start}, end={end}\")\n",
    "\n",
    "        spans.append(\n",
    "            {\n",
    "                \"assistant_turn\": turn_id,\n",
    "                \"message_index\": msg_idx,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"text\": content,\n",
    "            }\n",
    "        )\n",
    "        turn_id += 1\n",
    "\n",
    "    return spans\n",
    "\n",
    "\n",
    "def extract_single_pass(model, sae, tokenizer, conversation: list[dict], target_layer: int) -> dict:\n",
    "    full_ids = chat_token_ids(tokenizer, conversation, add_generation_prompt=False)\n",
    "    input_ids = torch.tensor(full_ids, dtype=torch.long, device=model.device).unsqueeze(0)\n",
    "\n",
    "    resid = gather_residual_activations(model, target_layer, input_ids)\n",
    "    sae_feats = sae.encode(resid.to(sae.dtype).to(sae.device)).squeeze(0)\n",
    "    full_features = sae_feats.detach().float().cpu().numpy()\n",
    "\n",
    "    if full_features.shape[0] != full_ids.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Token length mismatch: features={full_features.shape[0]}, ids={full_ids.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    spans = build_assistant_spans(tokenizer, conversation)\n",
    "    return {\n",
    "        \"full_token_ids\": full_ids,\n",
    "        \"full_features\": full_features,\n",
    "        \"assistant_spans\": spans,\n",
    "    }\n",
    "\n",
    "\n",
    "def pool_tokens(token_features: np.ndarray, pooling: str) -> np.ndarray:\n",
    "    if pooling == \"mean\":\n",
    "        return token_features.mean(axis=0)\n",
    "    if pooling == \"max\":\n",
    "        return token_features.max(axis=0)\n",
    "    raise ValueError(f\"Unknown pooling: {pooling}\")\n",
    "\n",
    "\n",
    "def build_turn_vectors(extraction: dict, pooling: str) -> tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"Build one pooled vector per assistant turn from single-pass extraction (assistant tokens only).\"\"\"\n",
    "    feats = extraction[\"full_features\"]\n",
    "    ids = extraction[\"full_token_ids\"]\n",
    "\n",
    "    rows = []\n",
    "    vectors = []\n",
    "\n",
    "    for span in extraction[\"assistant_spans\"]:\n",
    "        start, end = int(span[\"start\"]), int(span[\"end\"])\n",
    "        token_feats = feats[start:end]\n",
    "\n",
    "        x_t = pool_tokens(token_feats, pooling)\n",
    "        vectors.append(x_t)\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"assistant_turn\": int(span[\"assistant_turn\"]),\n",
    "                \"message_index\": int(span[\"message_index\"]),\n",
    "                \"span_start\": start,\n",
    "                \"span_end\": end,\n",
    "                \"pool_start\": start,\n",
    "                \"pool_end\": end,\n",
    "                \"n_tokens_pooled\": int(token_feats.shape[0]),\n",
    "                \"turn_l2_raw\": float(np.linalg.norm(x_t)),\n",
    "                \"turn_l1_raw\": float(np.abs(x_t).sum()),\n",
    "                \"turn_nonzero_raw\": int((x_t > 0).sum()),\n",
    "                \"assistant_text\": span[\"text\"],\n",
    "                \"_assistant_ids\": ids[start:end],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    X = np.stack(vectors, axis=0).astype(np.float32, copy=False)\n",
    "    meta = pd.DataFrame(rows)\n",
    "    return X, meta\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04202d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RUN_MODEL_EXTRACTION:\n",
    "    raise RuntimeError(\"Set RUN_MODEL_EXTRACTION=True to run extraction in this notebook\")\n",
    "\n",
    "extraction = extract_single_pass(model, sae, tokenizer, conversation, target_layer)\n",
    "turn_vectors_raw, turn_meta = build_turn_vectors(extraction, POOLING)\n",
    "\n",
    "print(f\"Turn vector matrix: {turn_vectors_raw.shape}\")\n",
    "print(turn_meta[[\"assistant_turn\", \"n_tokens_pooled\", \"turn_l2_raw\", \"turn_nonzero_raw\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Turn Scoring and Role Trace\n",
    "\n",
    "Each assistant turn is projected onto the reference axis to build a drift timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Build turn scores and nearest-role trace\n",
    "turn_vectors_space = apply_transform(turn_vectors_raw, TRANSFORM)\n",
    "if CENTER_TURNS_WITH_ROLE_MEAN:\n",
    "    turn_vectors_space = turn_vectors_space - role_mean_vec\n",
    "\n",
    "turn_vectors_unit = l2_normalize(turn_vectors_space, axis=1)\n",
    "\n",
    "# Score families\n",
    "turn_vectors_for_projection = turn_vectors_unit if USE_L2_NORMALIZE else turn_vectors_space\n",
    "projection = turn_vectors_for_projection @ axis_unit\n",
    "cos_axis = turn_vectors_unit @ axis_unit\n",
    "cos_assistant = turn_vectors_unit @ assistant_role_unit\n",
    "\n",
    "if SCORE_METHOD == \"projection\":\n",
    "    score = projection\n",
    "elif SCORE_METHOD == \"cos_axis\":\n",
    "    score = cos_axis\n",
    "elif SCORE_METHOD == \"cos_assistant\":\n",
    "    score = cos_assistant\n",
    "else:\n",
    "    raise ValueError(f\"Unknown SCORE_METHOD: {SCORE_METHOD}\")\n",
    "\n",
    "# Nearest roles per turn (direction context)\n",
    "turn_role_sim = turn_vectors_unit @ role_matrix_unit.T  # [T, R]\n",
    "topk_idx = np.argsort(turn_role_sim, axis=1)[:, ::-1][:, :TOP_K_ROLES]\n",
    "rows = []\n",
    "for t in range(topk_idx.shape[0]):\n",
    "    row = {\"assistant_turn\": t}\n",
    "    for k in range(TOP_K_ROLES):\n",
    "        ridx = int(topk_idx[t, k])\n",
    "        row[f\"top{k+1}_role\"] = role_names[ridx]\n",
    "        row[f\"top{k+1}_sim\"] = float(turn_role_sim[t, ridx])\n",
    "    rows.append(row)\n",
    "role_trace_df = pd.DataFrame(rows)\n",
    "\n",
    "# Attach diagnostics fields to turn metadata\n",
    "turn_meta = turn_meta.copy()\n",
    "turn_meta[\"score\"] = score\n",
    "turn_meta[\"projection\"] = projection\n",
    "turn_meta[\"cos_axis\"] = cos_axis\n",
    "turn_meta[\"cos_assistant\"] = cos_assistant\n",
    "turn_meta[\"turn_l2_space\"] = np.linalg.norm(turn_vectors_space, axis=1)\n",
    "turn_meta[\"turn_nonzero_space\"] = (turn_vectors_space > 0).sum(axis=1)\n",
    "\n",
    "print(f\"Scoring method: {SCORE_METHOD}\")\n",
    "display(turn_meta[[\"assistant_turn\", \"n_tokens_pooled\", \"score\", \"turn_l2_raw\", \"turn_nonzero_raw\", \"turn_l2_space\", \"turn_nonzero_space\"]])\n",
    "print(\"Nearest roles per turn:\")\n",
    "display(role_trace_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confound Diagnostics\n",
    "\n",
    "A score can move because activation mass changes, not only because persona direction changed.\n",
    "So we check score correlations with norm/sparsity metrics and flag how cautious interpretation should be.\n",
    "Note: I found that with the default settings in this notebook to correlation is moderate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Confound diagnostics for selected score only\n",
    "mass_metrics = [\"turn_l2_raw\", \"turn_nonzero_raw\", \"turn_l2_space\", \"turn_nonzero_space\"]\n",
    "\n",
    "selected_diag = pd.DataFrame(\n",
    "    {\n",
    "        \"metric\": mass_metrics,\n",
    "        \"score_method\": SCORE_METHOD,\n",
    "        \"corr(score, metric)\": [\n",
    "            corr_or_nan(turn_meta[\"score\"].values, turn_meta[m].values) for m in mass_metrics\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "score_mass_diag = selected_diag.copy()\n",
    "score_mass_pivot = score_mass_diag.pivot(index=\"metric\", columns=\"score_method\", values=\"corr(score, metric)\")\n",
    "\n",
    "vals = selected_diag[\"corr(score, metric)\"].values\n",
    "if np.all(np.isnan(vals)):\n",
    "    max_abs_corr = np.nan\n",
    "else:\n",
    "    max_abs_corr = float(np.nanmax(np.abs(vals)))\n",
    "\n",
    "if np.isnan(max_abs_corr):\n",
    "    confound_verdict = \"N/A\"\n",
    "elif max_abs_corr >= CONFOUND_HIGH_ABS_CORR:\n",
    "    confound_verdict = \"HIGH\"\n",
    "elif max_abs_corr >= CONFOUND_WARN_ABS_CORR:\n",
    "    confound_verdict = \"MODERATE\"\n",
    "else:\n",
    "    confound_verdict = \"LOW\"\n",
    "\n",
    "print(\"Confound diagnostics for selected score:\")\n",
    "display(selected_diag)\n",
    "print(f\"Confound verdict: {confound_verdict} (max |corr| = {max_abs_corr if not np.isnan(max_abs_corr) else 'nan'})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select One Transition\n",
    "\n",
    "Q2 and Q3 explain a single event (`t-1 -> t`).\n",
    "Default behavior picks the largest score drop, but manual selection is available when testing a specific hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assistant-axis style: select a single transition to explain\n",
    "if len(score) < 2:\n",
    "    raise RuntimeError(\"Need at least 2 assistant turns to explain a transition\")\n",
    "\n",
    "score_delta = np.diff(score)\n",
    "transition_df = pd.DataFrame(\n",
    "    {\n",
    "        \"turn\": np.arange(1, len(score)),\n",
    "        \"prev_turn\": np.arange(0, len(score) - 1),\n",
    "        \"delta\": score_delta,\n",
    "        \"score_prev\": score[:-1],\n",
    "        \"score_now\": score[1:],\n",
    "    }\n",
    ")\n",
    "\n",
    "if TRANSITION_PICK == \"largest_drop\":\n",
    "    t_now = int(np.argmin(score_delta) + 1)\n",
    "elif TRANSITION_PICK == \"manual\":\n",
    "    if MANUAL_TURN is None:\n",
    "        raise ValueError(\"Set MANUAL_TURN when TRANSITION_PICK='manual'\")\n",
    "    t_now = int(MANUAL_TURN)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown TRANSITION_PICK: {TRANSITION_PICK}\")\n",
    "\n",
    "if t_now <= 0 or t_now >= len(score):\n",
    "    raise ValueError(f\"Selected turn must be in [1, {len(score)-1}], got {t_now}\")\n",
    "\n",
    "print(\"Score transitions:\")\n",
    "display(transition_df)\n",
    "print(f\"Selected transition mode: {TRANSITION_PICK}\")\n",
    "print(f\"Explaining transition: turn {t_now - 1} -> {t_now}\")\n",
    "print(f\"Δscore (selected method) = {score[t_now] - score[t_now - 1]:.4f}\")\n",
    "print(f\"Δprojection = {projection[t_now] - projection[t_now - 1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a97f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 trajectory visualization (selected score method)\n",
    "turns = np.arange(len(score))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4.5))\n",
    "ax.plot(turns, score, marker=\"o\", linewidth=2.5, color=\"#1f77b4\")\n",
    "ax.axhline(0.0, color=\"gray\", linestyle=\"--\", linewidth=1.0, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel(\"assistant turn\")\n",
    "ax.set_ylabel(f\"score ({SCORE_METHOD})\")\n",
    "ax.set_title(f\"Q1: Persona drift trajectory ({SCORE_METHOD})\")\n",
    "ax.set_xticks(turns)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Feature-Level Drift Mechanism\n",
    "\n",
    "For the selected transition, compute feature deltas and weight them by the assistant axis.\n",
    "This distinguishes features that changed a lot from features that actually moved the persona score away or toward assistant behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature activation/de-activation and effect strength on selected transition\n",
    "if t_now <= 0:\n",
    "    raise RuntimeError(\"Need t_now > 0 for transition explanation\")\n",
    "\n",
    "x_prev = turn_vectors_space[t_now - 1]\n",
    "x_now = turn_vectors_space[t_now]\n",
    "delta_x = x_now - x_prev\n",
    "contrib = delta_x * axis_unit\n",
    "\n",
    "desc_cache = load_description_cache(NEURONPEDIA_CACHE if NEURONPEDIA_CACHE.exists() else None)\n",
    "\n",
    "\n",
    "def role_profile_for_feature(feature_idx: int, role_matrix: np.ndarray, role_names: np.ndarray, top_n: int = 3) -> str:\n",
    "    vals = role_matrix[:, feature_idx]\n",
    "    vals_pos = np.clip(vals, 0.0, None)\n",
    "    total = float(vals_pos.sum())\n",
    "    if total <= 0:\n",
    "        return \"no positive role mass\"\n",
    "    order = np.argsort(vals_pos)[::-1][:top_n]\n",
    "    return \" | \".join(f\"{role_names[i]}:{(vals_pos[i] / total):.0%}\" for i in order)\n",
    "\n",
    "\n",
    "feature_core_df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature_idx\": np.arange(delta_x.shape[0], dtype=int),\n",
    "        \"delta_x\": delta_x.astype(np.float64),\n",
    "        \"axis_weight\": axis_unit.astype(np.float64),\n",
    "        \"contrib\": contrib.astype(np.float64),\n",
    "    }\n",
    ")\n",
    "feature_core_df[\"direction\"] = np.where(feature_core_df[\"delta_x\"] > 0, \"activated\", \"deactivated\")\n",
    "feature_core_df[\"axis_side\"] = np.where(feature_core_df[\"axis_weight\"] > 0, \"assistant_like\", \"non_assistant_like\")\n",
    "feature_core_df[\"effect\"] = np.where(feature_core_df[\"contrib\"] < 0, \"away_from_assistant\", \"toward_assistant\")\n",
    "\n",
    "away_mass = float(np.clip(-feature_core_df[\"contrib\"].values, 0.0, None).sum())\n",
    "toward_mass = float(np.clip(feature_core_df[\"contrib\"].values, 0.0, None).sum())\n",
    "feature_core_df[\"share_of_away\"] = np.where(\n",
    "    feature_core_df[\"contrib\"] < 0,\n",
    "    (-feature_core_df[\"contrib\"]) / max(away_mass, 1e-12),\n",
    "    0.0,\n",
    ")\n",
    "\n",
    "\n",
    "def enrich_feature_table(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, row in df_in.iterrows():\n",
    "        fid = int(row[\"feature_idx\"])\n",
    "        cache = desc_cache.get(fid, {})\n",
    "        rows.append(\n",
    "            {\n",
    "                \"feature_idx\": fid,\n",
    "                \"delta_x\": float(row[\"delta_x\"]),\n",
    "                \"axis_weight\": float(row[\"axis_weight\"]),\n",
    "                \"contrib\": float(row[\"contrib\"]),\n",
    "                \"share_of_away\": float(row[\"share_of_away\"]),\n",
    "                \"direction\": row[\"direction\"],\n",
    "                \"axis_side\": row[\"axis_side\"],\n",
    "                \"effect\": row[\"effect\"],\n",
    "                \"role_profile\": role_profile_for_feature(fid, role_matrix_space, role_names),\n",
    "                \"description\": cache.get(\"description\"),\n",
    "                \"url\": cache.get(\"url\"),\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Core UI buckets\n",
    "mask_deactivated_assistant = (feature_core_df[\"delta_x\"] < 0) & (feature_core_df[\"axis_weight\"] > 0)\n",
    "mask_activated_nonassistant = (feature_core_df[\"delta_x\"] > 0) & (feature_core_df[\"axis_weight\"] < 0)\n",
    "\n",
    "df_deactivated_assistant = enrich_feature_table(\n",
    "    feature_core_df[mask_deactivated_assistant].sort_values(\"contrib\", ascending=True).head(TOP_FEATURES)\n",
    ")\n",
    "df_activated_nonassistant = enrich_feature_table(\n",
    "    feature_core_df[mask_activated_nonassistant].sort_values(\"contrib\", ascending=True).head(TOP_FEATURES)\n",
    ")\n",
    "\n",
    "# Additional context\n",
    "mask_away = feature_core_df[\"contrib\"] < 0\n",
    "mask_toward = feature_core_df[\"contrib\"] > 0\n",
    "\n",
    "df_top_away = enrich_feature_table(feature_core_df[mask_away].sort_values(\"contrib\", ascending=True).head(TOP_FEATURES))\n",
    "df_top_toward = enrich_feature_table(feature_core_df[mask_toward].sort_values(\"contrib\", ascending=False).head(TOP_FEATURES))\n",
    "\n",
    "if len(df_top_away) == 0:\n",
    "    top_away_share = 0.0\n",
    "else:\n",
    "    top_away_share = float(df_top_away[\"share_of_away\"].sum())\n",
    "\n",
    "if top_away_share >= 0.60:\n",
    "    effect_strength = \"STRONG\"\n",
    "elif top_away_share >= 0.35:\n",
    "    effect_strength = \"MODERATE\"\n",
    "else:\n",
    "    effect_strength = \"WEAK\"\n",
    "\n",
    "feature_event_summary = {\n",
    "    \"selected_turn\": int(t_now),\n",
    "    \"prev_turn\": int(t_now - 1),\n",
    "    \"delta_score\": float(score[t_now] - score[t_now - 1]),\n",
    "    \"away_mass\": away_mass,\n",
    "    \"toward_mass\": toward_mass,\n",
    "    \"top_away_share\": top_away_share,\n",
    "    \"effect_strength\": effect_strength,\n",
    "}\n",
    "\n",
    "print(\"Q2 summary:\")\n",
    "print(feature_event_summary)\n",
    "print(\"A) Assistant-like features that de-activated (away-driving)\")\n",
    "display(df_deactivated_assistant)\n",
    "print(\"B) Non-assistant-like features that activated (away-driving)\")\n",
    "display(df_activated_nonassistant)\n",
    "print(\"C) Top away contributors (all types)\")\n",
    "display(df_top_away)\n",
    "print(\"D) Top toward contributors\")\n",
    "display(df_top_toward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Token Localization\n",
    "\n",
    "After feature attribution, project token vectors from the selected turn onto the same axis.\n",
    "This shows which tokens carry assistant-like or non-assistant-like signal inside that message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c22c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Token-level localization on selected event turn\n",
    "span = extraction[\"assistant_spans\"][t_now]\n",
    "start, end = int(span[\"start\"]), int(span[\"end\"])\n",
    "token_ids = extraction[\"full_token_ids\"][start:end]\n",
    "token_feats = extraction[\"full_features\"][start:end]\n",
    "\n",
    "token_space = apply_transform(token_feats, TRANSFORM)\n",
    "if CENTER_TURNS_WITH_ROLE_MEAN:\n",
    "    token_space = token_space - role_mean_vec\n",
    "\n",
    "token_proj = token_space @ axis_unit\n",
    "lo, hi = np.percentile(token_proj, TOKEN_HEAT_PERCENTILES)\n",
    "if hi <= lo:\n",
    "    hi = lo + 1e-6\n",
    "token_norm = np.clip((token_proj - lo) / (hi - lo), 0.0, 1.0)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids.tolist())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(10, len(tokens) * 0.22), 3.5))\n",
    "ax.bar(np.arange(len(tokens)), token_proj, color=plt.cm.coolwarm(token_norm))\n",
    "ax.set_title(f\"Q3: token projection on axis (assistant turn {t_now})\")\n",
    "ax.set_xlabel(\"token index\")\n",
    "ax.set_ylabel(\"u_k\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compact text heat rendering\n",
    "cmap = plt.get_cmap(\"coolwarm\")\n",
    "spans = []\n",
    "for tok, val in zip(tokens, token_norm):\n",
    "    r, g, b, _ = cmap(float(val))\n",
    "    bg = f\"rgba({int(r*255)}, {int(g*255)}, {int(b*255)}, 0.70)\"\n",
    "    safe_tok = html.escape(tok).replace(\" \", \"&nbsp;\")\n",
    "    spans.append(\n",
    "        f\"<span style='background:{bg}; padding:2px 3px; margin:1px; border-radius:3px; display:inline-block'>{safe_tok}</span>\"\n",
    "    )\n",
    "\n",
    "display(HTML(\"\".join(spans)))\n",
    "\n",
    "localization_df = pd.DataFrame(\n",
    "    {\n",
    "        \"token_idx\": np.arange(len(tokens)),\n",
    "        \"token\": tokens,\n",
    "        \"u_k\": token_proj,\n",
    "        \"u_k_norm\": token_norm,\n",
    "    }\n",
    ")\n",
    "display(localization_df)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Explorer Payload\n",
    "\n",
    "Q1, Q2, and Q3 outputs are bundled into a compact JSON payload for the static explorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static UI payload: Q1 + per-transition Q2 + per-turn Q3\n",
    "import hashlib\n",
    "UI_TOP_FEATURES = int(TOP_FEATURES)\n",
    "if UI_TOP_FEATURES <= 0:\n",
    "    raise ValueError(\"TOP_FEATURES must be positive\")\n",
    "\n",
    "delta_all = turn_vectors_space[1:] - turn_vectors_space[:-1]\n",
    "contrib_all = delta_all * axis_unit.reshape(1, -1)\n",
    "\n",
    "# Q1 payload (single selected score method)\n",
    "q1_turns = [\n",
    "    {\n",
    "        \"assistant_turn\": int(t),\n",
    "        \"score\": float(score[t]),\n",
    "    }\n",
    "    for t in range(len(score))\n",
    "]\n",
    "\n",
    "q1_transitions = [\n",
    "    {\n",
    "        \"prev_turn\": int(t - 1),\n",
    "        \"turn\": int(t),\n",
    "        \"delta_score\": float(score[t] - score[t - 1]),\n",
    "    }\n",
    "    for t in range(1, len(score))\n",
    "]\n",
    "\n",
    "# Q2 payload (top activated/deactivated features per transition)\n",
    "q2_transitions = []\n",
    "for i in range(delta_all.shape[0]):\n",
    "    prev_turn = int(i)\n",
    "    turn = int(i + 1)\n",
    "\n",
    "    d = delta_all[i]\n",
    "    c = contrib_all[i]\n",
    "\n",
    "    abs_change_mass = float(np.abs(d).sum())\n",
    "    away_mass_i = float(np.clip(-c, 0.0, None).sum())\n",
    "    toward_mass_i = float(np.clip(c, 0.0, None).sum())\n",
    "\n",
    "    activated_idx = np.where(d > 0)[0]\n",
    "    deactivated_idx = np.where(d < 0)[0]\n",
    "\n",
    "    if activated_idx.size:\n",
    "        activated_rank = activated_idx[np.argsort(d[activated_idx])[::-1][:UI_TOP_FEATURES]]\n",
    "    else:\n",
    "        activated_rank = np.array([], dtype=int)\n",
    "\n",
    "    if deactivated_idx.size:\n",
    "        deactivated_rank = deactivated_idx[np.argsort(d[deactivated_idx])[:UI_TOP_FEATURES]]\n",
    "    else:\n",
    "        deactivated_rank = np.array([], dtype=int)\n",
    "\n",
    "    def _feature_entry(fid: int, direction: str) -> dict:\n",
    "        dx = float(d[fid])\n",
    "        aw = float(axis_unit[fid])\n",
    "        cb = float(c[fid])\n",
    "        return {\n",
    "            \"feature_idx\": int(fid),\n",
    "            \"direction\": direction,\n",
    "            \"delta_x\": dx,\n",
    "            \"axis_weight\": aw,\n",
    "            \"contrib\": cb,\n",
    "            \"axis_side\": \"assistant_like\" if aw > 0 else \"non_assistant_like\",\n",
    "            \"effect\": \"away_from_assistant\" if cb < 0 else \"toward_assistant\",\n",
    "            \"share_of_change\": float(abs(dx) / max(abs_change_mass, 1e-12)),\n",
    "            \"share_of_away\": float(max(-cb, 0.0) / max(away_mass_i, 1e-12)),\n",
    "            \"role_profile\": role_profile_for_feature(int(fid), role_matrix_space, role_names),\n",
    "        }\n",
    "\n",
    "    q2_transitions.append(\n",
    "        {\n",
    "            \"prev_turn\": prev_turn,\n",
    "            \"turn\": turn,\n",
    "            \"delta_score\": float(score[turn] - score[prev_turn]),\n",
    "            \"away_mass\": away_mass_i,\n",
    "            \"toward_mass\": toward_mass_i,\n",
    "            \"activated\": [_feature_entry(int(fid), \"activated\") for fid in activated_rank.tolist()],\n",
    "            \"deactivated\": [_feature_entry(int(fid), \"deactivated\") for fid in deactivated_rank.tolist()],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Q3 payload (token localization for every assistant turn)\n",
    "q3_turns = []\n",
    "for t, span in enumerate(extraction[\"assistant_spans\"]):\n",
    "    start, end = int(span[\"start\"]), int(span[\"end\"])\n",
    "    token_ids_t = extraction[\"full_token_ids\"][start:end]\n",
    "    token_feats_t = extraction[\"full_features\"][start:end]\n",
    "\n",
    "    token_space_t = apply_transform(token_feats_t, TRANSFORM)\n",
    "    if CENTER_TURNS_WITH_ROLE_MEAN:\n",
    "        token_space_t = token_space_t - role_mean_vec\n",
    "\n",
    "    token_proj_t = token_space_t @ axis_unit\n",
    "    lo_t, hi_t = np.percentile(token_proj_t, TOKEN_HEAT_PERCENTILES)\n",
    "    if hi_t <= lo_t:\n",
    "        hi_t = lo_t + 1e-6\n",
    "    token_norm_t = np.clip((token_proj_t - lo_t) / (hi_t - lo_t), 0.0, 1.0)\n",
    "\n",
    "    tokens_t = tokenizer.convert_ids_to_tokens(token_ids_t.tolist())\n",
    "    q3_turns.append(\n",
    "        {\n",
    "            \"assistant_turn\": int(t),\n",
    "            \"tokens\": [str(tok) for tok in tokens_t],\n",
    "            \"u_k\": token_proj_t.astype(np.float32).tolist(),\n",
    "            \"u_k_norm\": token_norm_t.astype(np.float32).tolist(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "conversation_hash = hashlib.sha1(json.dumps(conversation, sort_keys=True).encode(\"utf-8\")).hexdigest()[:10]\n",
    "\n",
    "static_ui_payload = {\n",
    "    \"version\": 1,\n",
    "    \"run_meta\": {\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"sae_id\": SAE_ID,\n",
    "        \"assistant_role\": ASSISTANT_ROLE,\n",
    "        \"score_method\": SCORE_METHOD,\n",
    "        \"conversation_hash\": conversation_hash,\n",
    "    },\n",
    "    \"q1\": {\n",
    "        \"turn_scores\": q1_turns,\n",
    "        \"transitions\": q1_transitions,\n",
    "    },\n",
    "    \"q2\": {\n",
    "        \"top_features_per_direction\": UI_TOP_FEATURES,\n",
    "        \"transitions\": q2_transitions,\n",
    "    },\n",
    "    \"q3\": {\n",
    "        \"token_localization\": q3_turns,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Built static_ui_payload: turns={len(q1_turns)}, transitions={len(q2_transitions)}, \"\n",
    "    f\"token_turns={len(q3_turns)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Run Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26950fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save run artifacts for downstream analysis (lightweight by default)\n",
    "from datetime import datetime, timezone\n",
    "import hashlib\n",
    "\n",
    "REQUIRED_VARS = [\n",
    "    \"score\", \"projection\", \"cos_axis\", \"cos_assistant\", \"turn_meta\", \"role_trace_df\",\n",
    "    \"score_mass_diag\", \"score_mass_pivot\", \"selected_diag\", \"max_abs_corr\", \"transition_df\",\n",
    "    \"t_now\", \"delta_x\", \"contrib\", \"df_top_away\", \"df_top_toward\",\n",
    "    \"df_deactivated_assistant\", \"df_activated_nonassistant\", \"feature_event_summary\",\n",
    "    \"localization_df\", \"turn_vectors_raw\", \"turn_vectors_space\", \"turn_vectors_unit\", \"axis_unit\",\n",
    "    \"token_proj\", \"token_norm\", \"static_ui_payload\",\n",
    "]\n",
    "missing = [name for name in REQUIRED_VARS if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Run all prior analysis cells first. Missing variables: {missing}\")\n",
    "\n",
    "if Path.cwd().name == \"notebooks\":\n",
    "    RESULTS_ROOT = Path(\"../persona_drift_runs\")\n",
    "else:\n",
    "    RESULTS_ROOT = Path(\"./persona_drift_runs\")\n",
    "\n",
    "RUN_NAME = None  # optional override, e.g. \"coding_conv_debug\"\n",
    "\n",
    "# Lightweight defaults: keep only summary + conversation + UI bundle\n",
    "SAVE_DEBUG_TABLES = False\n",
    "SAVE_DEBUG_ARRAYS = False\n",
    "\n",
    "\n",
    "def _py(x):\n",
    "    if isinstance(x, (np.integer, np.floating)):\n",
    "        if np.isnan(x):\n",
    "            return None\n",
    "        return x.item()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.tolist()\n",
    "    if isinstance(x, Path):\n",
    "        return str(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _to_records(df: pd.DataFrame) -> list[dict]:\n",
    "    out = []\n",
    "    for row in df.to_dict(orient=\"records\"):\n",
    "        out.append({k: _py(v) for k, v in row.items()})\n",
    "    return out\n",
    "\n",
    "\n",
    "def _confound_verdict(max_abs_corr_value: float | None) -> str:\n",
    "    if max_abs_corr_value is None or (isinstance(max_abs_corr_value, float) and np.isnan(max_abs_corr_value)):\n",
    "        return \"N/A\"\n",
    "    if max_abs_corr_value >= CONFOUND_HIGH_ABS_CORR:\n",
    "        return \"HIGH\"\n",
    "    if max_abs_corr_value >= CONFOUND_WARN_ABS_CORR:\n",
    "        return \"MODERATE\"\n",
    "    return \"LOW\"\n",
    "\n",
    "\n",
    "utc_now = datetime.now(timezone.utc)\n",
    "conv_hash = hashlib.sha1(json.dumps(conversation, sort_keys=True).encode(\"utf-8\")).hexdigest()[:10]\n",
    "default_run_name = f\"{MODEL_NAME.split('/')[-1]}_{utc_now.strftime('%Y%m%d_%H%M%S')}_{conv_hash}\"\n",
    "run_name = RUN_NAME if RUN_NAME else default_run_name\n",
    "run_dir = RESULTS_ROOT / run_name\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Save raw conversation\n",
    "(run_dir / \"conversation.json\").write_text(json.dumps({\"conversation\": conversation}, ensure_ascii=False, indent=2))\n",
    "\n",
    "max_abs_corr_py = _py(float(max_abs_corr)) if not np.isnan(max_abs_corr) else None\n",
    "confound_verdict = _confound_verdict(max_abs_corr_py)\n",
    "\n",
    "# 2) Save compact UI bundle in the SAME run folder\n",
    "(run_dir / \"ui_bundle.json\").write_text(\n",
    "    json.dumps(static_ui_payload, ensure_ascii=False, separators=(\",\", \":\"))\n",
    ")\n",
    "\n",
    "# 3) Save summary metadata\n",
    "summary = {\n",
    "    \"run_name\": run_name,\n",
    "    \"timestamp_utc\": utc_now.isoformat(),\n",
    "    \"config\": {\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"sae_release\": SAE_RELEASE,\n",
    "        \"sae_id\": SAE_ID,\n",
    "        \"target_layer\": _py(globals().get(\"target_layer\")),\n",
    "        \"pooling\": POOLING,\n",
    "        \"transform\": TRANSFORM,\n",
    "        \"use_centering\": USE_CENTERING,\n",
    "        \"center_turns_with_role_mean\": CENTER_TURNS_WITH_ROLE_MEAN,\n",
    "        \"use_l2_normalize\": USE_L2_NORMALIZE,\n",
    "        \"axis_method\": AXIS_METHOD,\n",
    "        \"other_group\": OTHER_GROUP,\n",
    "        \"assistant_role\": ASSISTANT_ROLE,\n",
    "        \"score_method\": SCORE_METHOD,\n",
    "        \"top_k_roles\": TOP_K_ROLES,\n",
    "        \"transition_pick\": TRANSITION_PICK,\n",
    "        \"manual_turn\": _py(MANUAL_TURN),\n",
    "        \"top_features\": TOP_FEATURES,\n",
    "        \"token_heat_percentiles\": list(TOKEN_HEAT_PERCENTILES),\n",
    "        \"confound_warn_abs_corr\": CONFOUND_WARN_ABS_CORR,\n",
    "        \"confound_high_abs_corr\": CONFOUND_HIGH_ABS_CORR,\n",
    "    },\n",
    "    \"selection\": {\n",
    "        \"selected_turn\": int(t_now),\n",
    "        \"selected_prev_turn\": int(t_now - 1),\n",
    "        \"selected_delta_score\": float(score[t_now] - score[t_now - 1]),\n",
    "        \"selected_delta_projection\": float(projection[t_now] - projection[t_now - 1]),\n",
    "    },\n",
    "    \"diagnostics\": {\n",
    "        \"max_abs_corr_selected_score\": max_abs_corr_py,\n",
    "        \"confound_verdict\": confound_verdict,\n",
    "        \"selected_score_diag\": _to_records(selected_diag),\n",
    "        \"score_mass_diag\": _to_records(score_mass_diag),\n",
    "        \"feature_event_summary\": {k: _py(v) for k, v in feature_event_summary.items()},\n",
    "    },\n",
    "    \"files\": {\n",
    "        \"conversation\": \"conversation.json\",\n",
    "        \"summary\": \"summary.json\",\n",
    "        \"ui_bundle\": \"ui_bundle.json\",\n",
    "    },\n",
    "}\n",
    "(run_dir / \"summary.json\").write_text(json.dumps(summary, ensure_ascii=False, indent=2))\n",
    "\n",
    "# Optional heavy/debug exports\n",
    "if SAVE_DEBUG_TABLES:\n",
    "    csv_dir = run_dir / \"tables\"\n",
    "    csv_dir.mkdir(exist_ok=True)\n",
    "    turn_meta.to_csv(csv_dir / \"turn_meta.csv\", index=False)\n",
    "    role_trace_df.to_csv(csv_dir / \"role_trace.csv\", index=False)\n",
    "    score_mass_diag.to_csv(csv_dir / \"score_mass_diag.csv\", index=False)\n",
    "    score_mass_pivot.reset_index().to_csv(csv_dir / \"score_mass_pivot.csv\", index=False)\n",
    "    selected_diag.to_csv(csv_dir / \"selected_diag.csv\", index=False)\n",
    "    transition_df.to_csv(csv_dir / \"transition_df.csv\", index=False)\n",
    "    df_top_away.to_csv(csv_dir / \"q2_top_away_contrib.csv\", index=False)\n",
    "    df_top_toward.to_csv(csv_dir / \"q2_top_toward_contrib.csv\", index=False)\n",
    "    df_deactivated_assistant.to_csv(csv_dir / \"q2_deactivated_assistant_features.csv\", index=False)\n",
    "    df_activated_nonassistant.to_csv(csv_dir / \"q2_activated_nonassistant_features.csv\", index=False)\n",
    "    localization_df.to_csv(csv_dir / \"q3_token_localization.csv\", index=False)\n",
    "\n",
    "if SAVE_DEBUG_ARRAYS:\n",
    "    np.savez_compressed(\n",
    "        run_dir / \"arrays.npz\",\n",
    "        score=score,\n",
    "        projection=projection,\n",
    "        cos_axis=cos_axis,\n",
    "        cos_assistant=cos_assistant,\n",
    "        score_delta=np.diff(score),\n",
    "        axis_unit=axis_unit,\n",
    "        turn_vectors_raw=turn_vectors_raw,\n",
    "        turn_vectors_space=turn_vectors_space,\n",
    "        turn_vectors_unit=turn_vectors_unit,\n",
    "        delta_x=delta_x,\n",
    "        contrib=contrib,\n",
    "        token_proj=token_proj,\n",
    "        token_norm=token_norm,\n",
    "        role_names=np.asarray(role_names),\n",
    "    )\n",
    "\n",
    "print(f\"Saved run artifacts to: {run_dir.resolve()}\")\n",
    "print(\"Saved lightweight files: conversation.json, summary.json, ui_bundle.json\")\n",
    "if SAVE_DEBUG_TABLES or SAVE_DEBUG_ARRAYS:\n",
    "    print(\"Debug exports enabled: additional files were saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
